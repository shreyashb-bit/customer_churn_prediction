{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Modeling with Sampling Techniques - Customer Churn Prediction\n",
    "\n",
    "This notebook implements and compares three classification algorithms with various sampling techniques:\n",
    "\n",
    "**Models:**\n",
    "1. Logistic Regression\n",
    "2. Random Forest\n",
    "3. XGBoost\n",
    "\n",
    "**Sampling Techniques:**\n",
    "- No Sampling (Baseline)\n",
    "- Random Oversampling\n",
    "- SMOTE (Synthetic Minority Oversampling Technique)\n",
    "- ADASYN (Adaptive Synthetic Sampling)\n",
    "- Random Undersampling\n",
    "- Tomek Links\n",
    "- NearMiss\n",
    "- SMOTE + Tomek Links (Combined)\n",
    "- SMOTE + ENN (Combined)\n",
    "\n",
    "**Optimization:**\n",
    "- Hyperparameter tuning using GridSearchCV and RandomizedSearchCV\n",
    "- Cross-validation\n",
    "- Comprehensive performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\DS lectures\\customer-churn-ds\\customer_churn_prediction\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "SRC = str(Path.cwd().parent)\n",
    "os.chdir(SRC)\n",
    "print(SRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from time import time\n",
    "import json\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, GridSearchCV, \n",
    "    RandomizedSearchCV, StratifiedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Imbalanced-learn for sampling\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization styles\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (100000, 11)\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure_months</th>\n",
       "      <th>monthly_charges</th>\n",
       "      <th>total_charges</th>\n",
       "      <th>contract_type</th>\n",
       "      <th>internet_service</th>\n",
       "      <th>support_calls</th>\n",
       "      <th>late_payments</th>\n",
       "      <th>churn</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>46.0000</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>63.6700</td>\n",
       "      <td>102.9600</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>38.0000</td>\n",
       "      <td>28.0000</td>\n",
       "      <td>93.6600</td>\n",
       "      <td>2830.1500</td>\n",
       "      <td>One year</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>48.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>122.3700</td>\n",
       "      <td>924.3600</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>DSL</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>58.0000</td>\n",
       "      <td>37.0000</td>\n",
       "      <td>66.4500</td>\n",
       "      <td>1990.9500</td>\n",
       "      <td>One year</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>37.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>57.3200</td>\n",
       "      <td>20.0300</td>\n",
       "      <td>One year</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id     age  tenure_months  monthly_charges  total_charges  \\\n",
       "0            1 46.0000         8.0000          63.6700       102.9600   \n",
       "1            2 38.0000        28.0000          93.6600      2830.1500   \n",
       "2            3 48.0000         5.0000         122.3700       924.3600   \n",
       "3            4 58.0000        37.0000          66.4500      1990.9500   \n",
       "4            5 37.0000         2.0000          57.3200        20.0300   \n",
       "\n",
       "    contract_type internet_service  support_calls  late_payments  churn  \\\n",
       "0  Month-to-month      Fiber optic         3.0000              0      0   \n",
       "1        One year      Fiber optic         0.0000              1      1   \n",
       "2  Month-to-month              DSL         1.0000              0      0   \n",
       "3        One year      Fiber optic         5.0000              2      0   \n",
       "4        One year      Fiber optic         0.0000              1      1   \n",
       "\n",
       "         Date  \n",
       "0  2025-01-01  \n",
       "1  2025-01-02  \n",
       "2  2025-01-03  \n",
       "3  2025-01-04  \n",
       "4  2025-01-05  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/raw/customer_churn_dataset_with_date.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Churn distribution:\n",
      "churn\n",
      "0    60748\n",
      "1    39252\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Churn rate: 39.25%\n",
      "Imbalance ratio: 1:1.55\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Drop non-predictive columns\n",
    "df_processed = df_processed.drop(['customer_id', 'Date'], axis=1, errors='ignore')\n",
    "\n",
    "# Separate features and target\n",
    "X = df_processed.drop('churn', axis=1)\n",
    "y = df_processed['churn']\n",
    "\n",
    "print(f\"\\nChurn distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\nChurn rate: {y.mean()*100:.2f}%\")\n",
    "print(f\"Imbalance ratio: 1:{(y==0).sum()/(y==1).sum():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features (6): ['age', 'tenure_months', 'monthly_charges', 'total_charges', 'support_calls', 'late_payments']\n",
      "Categorical features (2): ['contract_type', 'internet_service']\n"
     ]
    }
   ],
   "source": [
    "# Identify feature types\n",
    "numerical_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values handled\n",
      "Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "for col in numerical_features:\n",
    "    if X[col].isnull().sum() > 0:\n",
    "        X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "for col in categorical_features:\n",
    "    if X[col].isnull().sum() > 0:\n",
    "        X[col] = X[col].fillna(X[col].mode()[0])\n",
    "\n",
    "print(\"Missing values handled\")\n",
    "print(f\"Remaining missing values: {X.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded contract_type:\n",
      "  Original values: <StringArray>\n",
      "['Month-to-month', 'One year', 'Two year']\n",
      "Length: 3, dtype: str\n",
      "  Encoded values: [0 1 2]\n",
      "\n",
      "Encoded internet_service:\n",
      "  Original values: <StringArray>\n",
      "['Fiber optic', 'DSL', 'No']\n",
      "Length: 3, dtype: str\n",
      "  Encoded values: [1 0 2]\n",
      "\n",
      "Encoded dataset shape: (100000, 8)\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical features\n",
    "X_encoded = X.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    X_encoded[col] = le.fit_transform(X[col])\n",
    "    label_encoders[col] = le\n",
    "    print(f\"\\nEncoded {col}:\")\n",
    "    print(f\"  Original values: {X[col].unique()}\")\n",
    "    print(f\"  Encoded values: {X_encoded[col].unique()}\")\n",
    "\n",
    "print(f\"\\nEncoded dataset shape: {X_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (80000, 8)\n",
      "Test set size: (20000, 8)\n",
      "\n",
      "Training set churn rate: 39.25%\n",
      "Test set churn rate: 39.25%\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "print(f\"\\nTraining set churn rate: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"Test set churn rate: {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled successfully!\n"
     ]
    }
   ],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"Features scaled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Sampling Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling techniques defined:\n",
      "  - No Sampling\n",
      "  - Random Oversampling\n",
      "  - SMOTE\n",
      "  - ADASYN\n",
      "  - Random Undersampling\n",
      "  - Tomek Links\n",
      "  - NearMiss\n",
      "  - SMOTE + Tomek\n",
      "  - SMOTE + ENN\n"
     ]
    }
   ],
   "source": [
    "# Define all sampling techniques\n",
    "sampling_techniques = {\n",
    "    'No Sampling': None,\n",
    "    'Random Oversampling': RandomOverSampler(random_state=RANDOM_STATE),\n",
    "    'SMOTE': SMOTE(random_state=RANDOM_STATE),\n",
    "    'ADASYN': ADASYN(random_state=RANDOM_STATE),\n",
    "    'Random Undersampling': RandomUnderSampler(random_state=RANDOM_STATE),\n",
    "    'Tomek Links': TomekLinks(),\n",
    "    'NearMiss': NearMiss(version=1),\n",
    "    'SMOTE + Tomek': SMOTETomek(random_state=RANDOM_STATE),\n",
    "    'SMOTE + ENN': SMOTEENN(random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "print(\"Sampling techniques defined:\")\n",
    "for name in sampling_techniques.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply sampling\n",
    "def apply_sampling(X_train, y_train, sampler, technique_name):\n",
    "    \"\"\"\n",
    "    Apply sampling technique and return resampled data\n",
    "    \"\"\"\n",
    "    if sampler is None:\n",
    "        return X_train, y_train\n",
    "    \n",
    "    print(f\"\\nApplying {technique_name}...\")\n",
    "    print(f\"  Before: {len(y_train)} samples, Churn rate: {y_train.mean()*100:.2f}%\")\n",
    "    \n",
    "    X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"  After: {len(y_resampled)} samples, Churn rate: {y_resampled.mean()*100:.2f}%\")\n",
    "    print(f\"  Class distribution: {pd.Series(y_resampled).value_counts().to_dict()}\")\n",
    "    \n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate model\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name, sampling_name):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation\n",
    "    \"\"\"\n",
    "    # Training predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    # Test predictions\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Sampling': sampling_name,\n",
    "        \n",
    "        # Training metrics\n",
    "        'Train_Accuracy': accuracy_score(y_train, y_train_pred),\n",
    "        'Train_Precision': precision_score(y_train, y_train_pred, zero_division=0),\n",
    "        'Train_Recall': recall_score(y_train, y_train_pred, zero_division=0),\n",
    "        'Train_F1': f1_score(y_train, y_train_pred, zero_division=0),\n",
    "        'Train_ROC_AUC': roc_auc_score(y_train, y_train_proba),\n",
    "        \n",
    "        # Test metrics\n",
    "        'Test_Accuracy': accuracy_score(y_test, y_test_pred),\n",
    "        'Test_Precision': precision_score(y_test, y_test_pred, zero_division=0),\n",
    "        'Test_Recall': recall_score(y_test, y_test_pred, zero_division=0),\n",
    "        'Test_F1': f1_score(y_test, y_test_pred, zero_division=0),\n",
    "        'Test_ROC_AUC': roc_auc_score(y_test, y_test_proba),\n",
    "        'Test_AP': average_precision_score(y_test, y_test_proba)\n",
    "    }\n",
    "    \n",
    "    return results, y_test_pred, y_test_proba\n",
    "\n",
    "print(\"Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Baseline Logistic Regression (All Sampling Techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOGISTIC REGRESSION - BASELINE MODELS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Sampling: No Sampling\n",
      "================================================================================\n",
      "\n",
      "Results:\n",
      "  Test Accuracy: 0.6371\n",
      "  Test Precision: 0.5632\n",
      "  Test Recall: 0.3361\n",
      "  Test F1-Score: 0.4209\n",
      "  Test ROC-AUC: 0.6613\n",
      "\n",
      "================================================================================\n",
      "Sampling: Random Oversampling\n",
      "================================================================================\n",
      "\n",
      "Applying Random Oversampling...\n",
      "  Before: 80000 samples, Churn rate: 39.25%\n",
      "  After: 97196 samples, Churn rate: 50.00%\n",
      "  Class distribution: {0: 48598, 1: 48598}\n",
      "\n",
      "Results:\n",
      "  Test Accuracy: 0.6048\n",
      "  Test Precision: 0.4976\n",
      "  Test Recall: 0.7048\n",
      "  Test F1-Score: 0.5833\n",
      "  Test ROC-AUC: 0.6612\n",
      "\n",
      "================================================================================\n",
      "Sampling: SMOTE\n",
      "================================================================================\n",
      "\n",
      "Applying SMOTE...\n",
      "  Before: 80000 samples, Churn rate: 39.25%\n",
      "  After: 97196 samples, Churn rate: 50.00%\n",
      "  Class distribution: {0: 48598, 1: 48598}\n",
      "\n",
      "Results:\n",
      "  Test Accuracy: 0.6040\n",
      "  Test Precision: 0.4968\n",
      "  Test Recall: 0.7025\n",
      "  Test F1-Score: 0.5821\n",
      "  Test ROC-AUC: 0.6608\n",
      "\n",
      "================================================================================\n",
      "Sampling: ADASYN\n",
      "================================================================================\n",
      "\n",
      "Applying ADASYN...\n",
      "  Before: 80000 samples, Churn rate: 39.25%\n",
      "  After: 97250 samples, Churn rate: 50.03%\n",
      "  Class distribution: {1: 48652, 0: 48598}\n",
      "\n",
      "Results:\n",
      "  Test Accuracy: 0.6005\n",
      "  Test Precision: 0.4938\n",
      "  Test Recall: 0.7178\n",
      "  Test F1-Score: 0.5851\n",
      "  Test ROC-AUC: 0.6608\n",
      "\n",
      "================================================================================\n",
      "Sampling: Random Undersampling\n",
      "================================================================================\n",
      "\n",
      "Applying Random Undersampling...\n",
      "  Before: 80000 samples, Churn rate: 39.25%\n",
      "  After: 62804 samples, Churn rate: 50.00%\n",
      "  Class distribution: {0: 31402, 1: 31402}\n",
      "\n",
      "Results:\n",
      "  Test Accuracy: 0.6042\n",
      "  Test Precision: 0.4971\n",
      "  Test Recall: 0.7027\n",
      "  Test F1-Score: 0.5823\n",
      "  Test ROC-AUC: 0.6610\n",
      "\n",
      "================================================================================\n",
      "Sampling: Tomek Links\n",
      "================================================================================\n",
      "\n",
      "Applying Tomek Links...\n",
      "  Before: 80000 samples, Churn rate: 39.25%\n",
      "  After: 70747 samples, Churn rate: 44.39%\n",
      "  Class distribution: {0: 39345, 1: 31402}\n",
      "\n",
      "Results:\n",
      "  Test Accuracy: 0.6330\n",
      "  Test Precision: 0.5295\n",
      "  Test Recall: 0.5824\n",
      "  Test F1-Score: 0.5547\n",
      "  Test ROC-AUC: 0.6611\n",
      "\n",
      "================================================================================\n",
      "Sampling: NearMiss\n",
      "================================================================================\n",
      "\n",
      "Applying NearMiss...\n",
      "  Before: 80000 samples, Churn rate: 39.25%\n",
      "  After: 62804 samples, Churn rate: 50.00%\n",
      "  Class distribution: {0: 31402, 1: 31402}\n",
      "\n",
      "Results:\n",
      "  Test Accuracy: 0.5613\n",
      "  Test Precision: 0.4548\n",
      "  Test Recall: 0.5929\n",
      "  Test F1-Score: 0.5148\n",
      "  Test ROC-AUC: 0.5863\n",
      "\n",
      "================================================================================\n",
      "Sampling: SMOTE + Tomek\n",
      "================================================================================\n",
      "\n",
      "Applying SMOTE + Tomek...\n",
      "  Before: 80000 samples, Churn rate: 39.25%\n",
      "  After: 84700 samples, Churn rate: 50.00%\n",
      "  Class distribution: {0: 42350, 1: 42350}\n",
      "\n",
      "Results:\n",
      "  Test Accuracy: 0.6042\n",
      "  Test Precision: 0.4970\n",
      "  Test Recall: 0.7048\n",
      "  Test F1-Score: 0.5830\n",
      "  Test ROC-AUC: 0.6609\n",
      "\n",
      "================================================================================\n",
      "Sampling: SMOTE + ENN\n",
      "================================================================================\n",
      "\n",
      "Applying SMOTE + ENN...\n",
      "  Before: 80000 samples, Churn rate: 39.25%\n",
      "  After: 34267 samples, Churn rate: 55.28%\n",
      "  Class distribution: {1: 18944, 0: 15323}\n",
      "\n",
      "Results:\n",
      "  Test Accuracy: 0.5845\n",
      "  Test Precision: 0.4815\n",
      "  Test Recall: 0.7632\n",
      "  Test F1-Score: 0.5904\n",
      "  Test ROC-AUC: 0.6614\n",
      "\n",
      "================================================================================\n",
      "LOGISTIC REGRESSION SUMMARY\n",
      "================================================================================\n",
      "               Sampling  Test_Accuracy  Test_Precision  Test_Recall  Test_F1  \\\n",
      "0           No Sampling         0.6371          0.5632       0.3361   0.4209   \n",
      "1   Random Oversampling         0.6048          0.4976       0.7048   0.5833   \n",
      "2                 SMOTE         0.6040          0.4968       0.7025   0.5821   \n",
      "3                ADASYN         0.6005          0.4938       0.7178   0.5851   \n",
      "4  Random Undersampling         0.6042          0.4971       0.7027   0.5823   \n",
      "5           Tomek Links         0.6330          0.5295       0.5824   0.5547   \n",
      "6              NearMiss         0.5613          0.4548       0.5929   0.5148   \n",
      "7         SMOTE + Tomek         0.6042          0.4970       0.7048   0.5830   \n",
      "8           SMOTE + ENN         0.5845          0.4815       0.7632   0.5904   \n",
      "\n",
      "   Test_ROC_AUC  \n",
      "0        0.6613  \n",
      "1        0.6612  \n",
      "2        0.6608  \n",
      "3        0.6608  \n",
      "4        0.6610  \n",
      "5        0.6611  \n",
      "6        0.5863  \n",
      "7        0.6609  \n",
      "8        0.6614  \n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression with all sampling techniques\n",
    "print(\"=\"*80)\n",
    "print(\"LOGISTIC REGRESSION - BASELINE MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "lr_results = []\n",
    "\n",
    "for sampling_name, sampler in sampling_techniques.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Sampling: {sampling_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Apply sampling\n",
    "    X_train_resampled, y_train_resampled = apply_sampling(\n",
    "        X_train_scaled, y_train, sampler, sampling_name\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    lr_model = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "    lr_model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Evaluate\n",
    "    results, _, _ = evaluate_model(\n",
    "        lr_model, X_train_resampled, y_train_resampled, \n",
    "        X_test_scaled, y_test, 'Logistic Regression', sampling_name\n",
    "    )\n",
    "    lr_results.append(results)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Test Accuracy: {results['Test_Accuracy']:.4f}\")\n",
    "    print(f\"  Test Precision: {results['Test_Precision']:.4f}\")\n",
    "    print(f\"  Test Recall: {results['Test_Recall']:.4f}\")\n",
    "    print(f\"  Test F1-Score: {results['Test_F1']:.4f}\")\n",
    "    print(f\"  Test ROC-AUC: {results['Test_ROC_AUC']:.4f}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "lr_results_df = pd.DataFrame(lr_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOGISTIC REGRESSION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(lr_results_df[['Sampling', 'Test_Accuracy', 'Test_Precision', \n",
    "                       'Test_Recall', 'Test_F1', 'Test_ROC_AUC']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Hyperparameter Tuning - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best sampling technique for Logistic Regression: SMOTE + ENN\n",
      "Best F1-Score: 0.5904\n",
      "\n",
      "Applying SMOTE + ENN...\n",
      "  Before: 80000 samples, Churn rate: 39.25%\n",
      "  After: 34267 samples, Churn rate: 55.28%\n",
      "  Class distribution: {1: 18944, 0: 15323}\n",
      "\n",
      "Hyperparameter Tuning for Logistic Regression...\n",
      "================================================================================\n",
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
      "\n",
      "Best parameters: {'C': 0.001, 'class_weight': 'balanced', 'max_iter': 1000, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "Best CV F1-Score: 0.8214\n",
      "\n",
      "Tuned Model Results:\n",
      "  Test Accuracy: 0.5933\n",
      "  Test Precision: 0.4882\n",
      "  Test Recall: 0.7478\n",
      "  Test F1-Score: 0.5907\n",
      "  Test ROC-AUC: 0.6621\n"
     ]
    }
   ],
   "source": [
    "# Find best sampling technique for Logistic Regression\n",
    "best_lr_sampling = lr_results_df.loc[lr_results_df['Test_F1'].idxmax(), 'Sampling']\n",
    "print(f\"Best sampling technique for Logistic Regression: {best_lr_sampling}\")\n",
    "print(f\"Best F1-Score: {lr_results_df['Test_F1'].max():.4f}\")\n",
    "\n",
    "# Apply best sampling\n",
    "best_sampler_lr = sampling_techniques[best_lr_sampling]\n",
    "X_train_lr_best, y_train_lr_best = apply_sampling(\n",
    "    X_train_scaled, y_train, best_sampler_lr, best_lr_sampling\n",
    ")\n",
    "\n",
    "# Hyperparameter tuning\n",
    "print(\"\\nHyperparameter Tuning for Logistic Regression...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "param_grid_lr = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'max_iter': [1000, 2000]\n",
    "}\n",
    "\n",
    "# Grid Search with cross-validation\n",
    "lr_grid = GridSearchCV(\n",
    "    LogisticRegression(random_state=RANDOM_STATE),\n",
    "    param_grid_lr,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_grid.fit(X_train_lr_best, y_train_lr_best)\n",
    "\n",
    "print(f\"\\nBest parameters: {lr_grid.best_params_}\")\n",
    "print(f\"Best CV F1-Score: {lr_grid.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate tuned model\n",
    "best_lr_model = lr_grid.best_estimator_\n",
    "lr_tuned_results, lr_tuned_pred, lr_tuned_proba = evaluate_model(\n",
    "    best_lr_model, X_train_lr_best, y_train_lr_best,\n",
    "    X_test_scaled, y_test, 'Logistic Regression (Tuned)', best_lr_sampling\n",
    ")\n",
    "\n",
    "print(\"\\nTuned Model Results:\")\n",
    "print(f\"  Test Accuracy: {lr_tuned_results['Test_Accuracy']:.4f}\")\n",
    "print(f\"  Test Precision: {lr_tuned_results['Test_Precision']:.4f}\")\n",
    "print(f\"  Test Recall: {lr_tuned_results['Test_Recall']:.4f}\")\n",
    "print(f\"  Test F1-Score: {lr_tuned_results['Test_F1']:.4f}\")\n",
    "print(f\"  Test ROC-AUC: {lr_tuned_results['Test_ROC_AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random Forest Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Baseline Random Forest (All Sampling Techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RANDOM FOREST - BASELINE MODELS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Sampling: No Sampling\n",
      "================================================================================\n",
      "\n",
      "Results:\n",
      "  Test Accuracy: 0.6341\n",
      "  Test Precision: 0.5436\n",
      "  Test Recall: 0.4229\n",
      "  Test F1-Score: 0.4757\n",
      "  Test ROC-AUC: 0.6585\n",
      "\n",
      "================================================================================\n",
      "Sampling: Random Oversampling\n",
      "================================================================================\n",
      "\n",
      "Applying Random Oversampling...\n",
      "  Before: 80000 samples, Churn rate: 39.25%\n",
      "  After: 97196 samples, Churn rate: 50.00%\n",
      "  Class distribution: {0: 48598, 1: 48598}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m     19\u001b[39m rf_model = RandomForestClassifier(\n\u001b[32m     20\u001b[39m     n_estimators=\u001b[32m100\u001b[39m, \n\u001b[32m     21\u001b[39m     random_state=RANDOM_STATE,\n\u001b[32m     22\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m\n\u001b[32m     23\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mrf_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_resampled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     27\u001b[39m results, _, _ = evaluate_model(\n\u001b[32m     28\u001b[39m     rf_model, X_train_resampled, y_train_resampled,\n\u001b[32m     29\u001b[39m     X_test, y_test, \u001b[33m'\u001b[39m\u001b[33mRandom Forest\u001b[39m\u001b[33m'\u001b[39m, sampling_name\n\u001b[32m     30\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\customer_churn_prediction\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\customer_churn_prediction\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:486\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    475\u001b[39m trees = [\n\u001b[32m    476\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    478\u001b[39m ]\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    508\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\customer_churn_prediction\\Lib\\site-packages\\sklearn\\utils\\parallel.py:91\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     79\u001b[39m warning_filters = (\n\u001b[32m     80\u001b[39m     filters_func() \u001b[38;5;28;01mif\u001b[39;00m filters_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m warnings.filters\n\u001b[32m     81\u001b[39m )\n\u001b[32m     83\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     84\u001b[39m     (\n\u001b[32m     85\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     90\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\customer_churn_prediction\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\customer_churn_prediction\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\customer_churn_prediction\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train Random Forest with all sampling techniques\n",
    "print(\"=\"*80)\n",
    "print(\"RANDOM FOREST - BASELINE MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "rf_results = []\n",
    "\n",
    "for sampling_name, sampler in sampling_techniques.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Sampling: {sampling_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Apply sampling\n",
    "    X_train_resampled, y_train_resampled = apply_sampling(\n",
    "        X_train, y_train, sampler, sampling_name\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Evaluate\n",
    "    results, _, _ = evaluate_model(\n",
    "        rf_model, X_train_resampled, y_train_resampled,\n",
    "        X_test, y_test, 'Random Forest', sampling_name\n",
    "    )\n",
    "    rf_results.append(results)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Test Accuracy: {results['Test_Accuracy']:.4f}\")\n",
    "    print(f\"  Test Precision: {results['Test_Precision']:.4f}\")\n",
    "    print(f\"  Test Recall: {results['Test_Recall']:.4f}\")\n",
    "    print(f\"  Test F1-Score: {results['Test_F1']:.4f}\")\n",
    "    print(f\"  Test ROC-AUC: {results['Test_ROC_AUC']:.4f}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "rf_results_df = pd.DataFrame(rf_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RANDOM FOREST SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(rf_results_df[['Sampling', 'Test_Accuracy', 'Test_Precision',\n",
    "                       'Test_Recall', 'Test_F1', 'Test_ROC_AUC']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Hyperparameter Tuning - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best sampling technique for Random Forest\n",
    "best_rf_sampling = rf_results_df.loc[rf_results_df['Test_F1'].idxmax(), 'Sampling']\n",
    "print(f\"Best sampling technique for Random Forest: {best_rf_sampling}\")\n",
    "print(f\"Best F1-Score: {rf_results_df['Test_F1'].max():.4f}\")\n",
    "\n",
    "# Apply best sampling\n",
    "best_sampler_rf = sampling_techniques[best_rf_sampling]\n",
    "X_train_rf_best, y_train_rf_best = apply_sampling(\n",
    "    X_train, y_train, best_sampler_rf, best_rf_sampling\n",
    ")\n",
    "\n",
    "# Hyperparameter tuning with RandomizedSearchCV for efficiency\n",
    "print(\"\\nHyperparameter Tuning for Random Forest...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True, False],\n",
    "    'class_weight': [None, 'balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "# Randomized Search\n",
    "rf_random = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    param_dist_rf,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "rf_random.fit(X_train_rf_best, y_train_rf_best)\n",
    "\n",
    "print(f\"\\nBest parameters: {rf_random.best_params_}\")\n",
    "print(f\"Best CV F1-Score: {rf_random.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate tuned model\n",
    "best_rf_model = rf_random.best_estimator_\n",
    "rf_tuned_results, rf_tuned_pred, rf_tuned_proba = evaluate_model(\n",
    "    best_rf_model, X_train_rf_best, y_train_rf_best,\n",
    "    X_test, y_test, 'Random Forest (Tuned)', best_rf_sampling\n",
    ")\n",
    "\n",
    "print(\"\\nTuned Model Results:\")\n",
    "print(f\"  Test Accuracy: {rf_tuned_results['Test_Accuracy']:.4f}\")\n",
    "print(f\"  Test Precision: {rf_tuned_results['Test_Precision']:.4f}\")\n",
    "print(f\"  Test Recall: {rf_tuned_results['Test_Recall']:.4f}\")\n",
    "print(f\"  Test F1-Score: {rf_tuned_results['Test_F1']:.4f}\")\n",
    "print(f\"  Test ROC-AUC: {rf_tuned_results['Test_ROC_AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. XGBoost Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Baseline XGBoost (All Sampling Techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost with all sampling techniques\n",
    "print(\"=\"*80)\n",
    "print(\"XGBOOST - BASELINE MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "xgb_results = []\n",
    "\n",
    "for sampling_name, sampler in sampling_techniques.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Sampling: {sampling_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Apply sampling\n",
    "    X_train_resampled, y_train_resampled = apply_sampling(\n",
    "        X_train, y_train, sampler, sampling_name\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    xgb_model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=RANDOM_STATE,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Evaluate\n",
    "    results, _, _ = evaluate_model(\n",
    "        xgb_model, X_train_resampled, y_train_resampled,\n",
    "        X_test, y_test, 'XGBoost', sampling_name\n",
    "    )\n",
    "    xgb_results.append(results)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Test Accuracy: {results['Test_Accuracy']:.4f}\")\n",
    "    print(f\"  Test Precision: {results['Test_Precision']:.4f}\")\n",
    "    print(f\"  Test Recall: {results['Test_Recall']:.4f}\")\n",
    "    print(f\"  Test F1-Score: {results['Test_F1']:.4f}\")\n",
    "    print(f\"  Test ROC-AUC: {results['Test_ROC_AUC']:.4f}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "xgb_results_df = pd.DataFrame(xgb_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"XGBOOST SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(xgb_results_df[['Sampling', 'Test_Accuracy', 'Test_Precision',\n",
    "                        'Test_Recall', 'Test_F1', 'Test_ROC_AUC']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Hyperparameter Tuning - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best sampling technique for XGBoost\n",
    "best_xgb_sampling = xgb_results_df.loc[xgb_results_df['Test_F1'].idxmax(), 'Sampling']\n",
    "print(f\"Best sampling technique for XGBoost: {best_xgb_sampling}\")\n",
    "print(f\"Best F1-Score: {xgb_results_df['Test_F1'].max():.4f}\")\n",
    "\n",
    "# Apply best sampling\n",
    "best_sampler_xgb = sampling_techniques[best_xgb_sampling]\n",
    "X_train_xgb_best, y_train_xgb_best = apply_sampling(\n",
    "    X_train, y_train, best_sampler_xgb, best_xgb_sampling\n",
    ")\n",
    "\n",
    "# Hyperparameter tuning with RandomizedSearchCV\n",
    "print(\"\\nHyperparameter Tuning for XGBoost...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "param_dist_xgb = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.3],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.5, 1],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'scale_pos_weight': [1, 2, 3],  # For imbalanced data\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [0, 0.1, 1]\n",
    "}\n",
    "\n",
    "# Randomized Search\n",
    "xgb_random = RandomizedSearchCV(\n",
    "    XGBClassifier(\n",
    "        random_state=RANDOM_STATE,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False\n",
    "    ),\n",
    "    param_dist_xgb,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "xgb_random.fit(X_train_xgb_best, y_train_xgb_best)\n",
    "\n",
    "print(f\"\\nBest parameters: {xgb_random.best_params_}\")\n",
    "print(f\"Best CV F1-Score: {xgb_random.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate tuned model\n",
    "best_xgb_model = xgb_random.best_estimator_\n",
    "xgb_tuned_results, xgb_tuned_pred, xgb_tuned_proba = evaluate_model(\n",
    "    best_xgb_model, X_train_xgb_best, y_train_xgb_best,\n",
    "    X_test, y_test, 'XGBoost (Tuned)', best_xgb_sampling\n",
    ")\n",
    "\n",
    "print(\"\\nTuned Model Results:\")\n",
    "print(f\"  Test Accuracy: {xgb_tuned_results['Test_Accuracy']:.4f}\")\n",
    "print(f\"  Test Precision: {xgb_tuned_results['Test_Precision']:.4f}\")\n",
    "print(f\"  Test Recall: {xgb_tuned_results['Test_Recall']:.4f}\")\n",
    "print(f\"  Test F1-Score: {xgb_tuned_results['Test_F1']:.4f}\")\n",
    "print(f\"  Test ROC-AUC: {xgb_tuned_results['Test_ROC_AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = pd.concat([\n",
    "    lr_results_df,\n",
    "    rf_results_df,\n",
    "    xgb_results_df\n",
    "], ignore_index=True)\n",
    "\n",
    "# Add tuned models\n",
    "all_results = pd.concat([\n",
    "    all_results,\n",
    "    pd.DataFrame([lr_tuned_results, rf_tuned_results, xgb_tuned_results])\n",
    "], ignore_index=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAll Models Performance:\")\n",
    "print(all_results[['Model', 'Sampling', 'Test_Accuracy', 'Test_Precision',\n",
    "                    'Test_Recall', 'Test_F1', 'Test_ROC_AUC']].sort_values(\n",
    "                        'Test_F1', ascending=False\n",
    "                    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best overall model\n",
    "best_model_idx = all_results['Test_F1'].idxmax()\n",
    "best_model_info = all_results.iloc[best_model_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST OVERALL MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {best_model_info['Model']}\")\n",
    "print(f\"Sampling: {best_model_info['Sampling']}\")\n",
    "print(f\"\\nTest Performance:\")\n",
    "print(f\"  Accuracy: {best_model_info['Test_Accuracy']:.4f}\")\n",
    "print(f\"  Precision: {best_model_info['Test_Precision']:.4f}\")\n",
    "print(f\"  Recall: {best_model_info['Test_Recall']:.4f}\")\n",
    "print(f\"  F1-Score: {best_model_info['Test_F1']:.4f}\")\n",
    "print(f\"  ROC-AUC: {best_model_info['Test_ROC_AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization - Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of all models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "metrics = ['Test_Accuracy', 'Test_Precision', 'Test_Recall', 'Test_F1', 'Test_ROC_AUC']\n",
    "metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "\n",
    "for idx, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    \n",
    "    # Group by model and sampling\n",
    "    pivot_data = all_results.pivot_table(\n",
    "        values=metric,\n",
    "        index='Sampling',\n",
    "        columns='Model',\n",
    "        aggfunc='first'\n",
    "    )\n",
    "    \n",
    "    pivot_data.plot(kind='bar', ax=axes[row, col], width=0.8)\n",
    "    axes[row, col].set_title(f'{name} Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[row, col].set_xlabel('Sampling Technique', fontsize=10)\n",
    "    axes[row, col].set_ylabel(name, fontsize=10)\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "    axes[row, col].legend(loc='best', fontsize=8)\n",
    "    axes[row, col].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes[1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of F1-Scores\n",
    "plt.figure(figsize=(14, 8))\n",
    "f1_heatmap = all_results.pivot_table(\n",
    "    values='Test_F1',\n",
    "    index='Sampling',\n",
    "    columns='Model',\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "sns.heatmap(f1_heatmap, annot=True, fmt='.4f', cmap='RdYlGn', \n",
    "            center=f1_heatmap.values.mean(), cbar_kws={'label': 'F1-Score'})\n",
    "plt.title('F1-Score Heatmap: Models vs Sampling Techniques', \n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('Sampling Technique', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrices for Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for tuned models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "models_cm = [\n",
    "    ('Logistic Regression', lr_tuned_pred),\n",
    "    ('Random Forest', rf_tuned_pred),\n",
    "    ('XGBoost', xgb_tuned_pred)\n",
    "]\n",
    "\n",
    "for idx, (name, predictions) in enumerate(models_cm):\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['No Churn', 'Churn'],\n",
    "                yticklabels=['No Churn', 'Churn'])\n",
    "    axes[idx].set_title(f'{name} (Tuned)\\nConfusion Matrix', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predicted', fontsize=10)\n",
    "    axes[idx].set_ylabel('Actual', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves for tuned models\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "models_roc = [\n",
    "    ('Logistic Regression (Tuned)', lr_tuned_proba, lr_tuned_results['Test_ROC_AUC']),\n",
    "    ('Random Forest (Tuned)', rf_tuned_proba, rf_tuned_results['Test_ROC_AUC']),\n",
    "    ('XGBoost (Tuned)', xgb_tuned_proba, xgb_tuned_results['Test_ROC_AUC'])\n",
    "]\n",
    "\n",
    "for name, proba, auc_score in models_roc:\n",
    "    fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC = {auc_score:.4f})')\n",
    "\n",
    "# Plot diagonal\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Tuned Models', fontsize=16, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curves for tuned models\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for name, proba, _ in models_roc:\n",
    "    precision, recall, _ = precision_recall_curve(y_test, proba)\n",
    "    ap_score = average_precision_score(y_test, proba)\n",
    "    plt.plot(recall, precision, linewidth=2, \n",
    "             label=f'{name} (AP = {ap_score:.4f})')\n",
    "\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curves - Tuned Models', fontsize=16, fontweight='bold')\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Feature Importance (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from best tree-based model\n",
    "# Compare RF and XGBoost feature importance\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Random Forest feature importance\n",
    "rf_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': best_rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(10)\n",
    "\n",
    "axes[0].barh(rf_importance_df['Feature'], rf_importance_df['Importance'], \n",
    "             color='forestgreen', edgecolor='black')\n",
    "axes[0].set_xlabel('Importance', fontsize=12)\n",
    "axes[0].set_ylabel('Features', fontsize=12)\n",
    "axes[0].set_title('Random Forest - Top 10 Features', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# XGBoost feature importance\n",
    "xgb_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': best_xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False).head(10)\n",
    "\n",
    "axes[1].barh(xgb_importance_df['Feature'], xgb_importance_df['Importance'],\n",
    "             color='dodgerblue', edgecolor='black')\n",
    "axes[1].set_xlabel('Importance', fontsize=12)\n",
    "axes[1].set_ylabel('Features', fontsize=12)\n",
    "axes[1].set_title('XGBoost - Top 10 Features', fontsize=14, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Results and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to CSV\n",
    "all_results.to_csv('model_comparison_results.csv', index=False)\n",
    "print(\"All results saved to 'model_comparison_results.csv'\")\n",
    "\n",
    "# Save best models\n",
    "import pickle\n",
    "\n",
    "models_to_save = {\n",
    "    'logistic_regression': best_lr_model,\n",
    "    'random_forest': best_rf_model,\n",
    "    'xgboost': best_xgb_model,\n",
    "    'scaler': scaler,\n",
    "    'label_encoders': label_encoders\n",
    "}\n",
    "\n",
    "for name, model in models_to_save.items():\n",
    "    with open(f'{name}_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"{name} saved\")\n",
    "\n",
    "print(\"\\nAll models saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODELING SUMMARY AND INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATASET CHARACTERISTICS:\")\n",
    "print(f\"   - Total samples: {len(df):,}\")\n",
    "print(f\"   - Training samples: {len(X_train):,}\")\n",
    "print(f\"   - Test samples: {len(X_test):,}\")\n",
    "print(f\"   - Original churn rate: {y.mean()*100:.2f}%\")\n",
    "print(f\"   - Class imbalance ratio: 1:{(y==0).sum()/(y==1).sum():.2f}\")\n",
    "\n",
    "print(\"\\n2. MODELS EVALUATED:\")\n",
    "print(\"   - Logistic Regression\")\n",
    "print(\"   - Random Forest\")\n",
    "print(\"   - XGBoost\")\n",
    "\n",
    "print(\"\\n3. SAMPLING TECHNIQUES TESTED:\")\n",
    "for name in sampling_techniques.keys():\n",
    "    print(f\"   - {name}\")\n",
    "\n",
    "print(\"\\n4. BEST SAMPLING TECHNIQUES BY MODEL:\")\n",
    "print(f\"   - Logistic Regression: {best_lr_sampling}\")\n",
    "print(f\"   - Random Forest: {best_rf_sampling}\")\n",
    "print(f\"   - XGBoost: {best_xgb_sampling}\")\n",
    "\n",
    "print(\"\\n5. BEST OVERALL MODEL:\")\n",
    "print(f\"   Model: {best_model_info['Model']}\")\n",
    "print(f\"   Sampling: {best_model_info['Sampling']}\")\n",
    "print(f\"   Test F1-Score: {best_model_info['Test_F1']:.4f}\")\n",
    "print(f\"   Test ROC-AUC: {best_model_info['Test_ROC_AUC']:.4f}\")\n",
    "\n",
    "print(\"\\n6. TUNED MODELS PERFORMANCE:\")\n",
    "print(f\"   Logistic Regression:\")\n",
    "print(f\"     - F1-Score: {lr_tuned_results['Test_F1']:.4f}\")\n",
    "print(f\"     - ROC-AUC: {lr_tuned_results['Test_ROC_AUC']:.4f}\")\n",
    "print(f\"   Random Forest:\")\n",
    "print(f\"     - F1-Score: {rf_tuned_results['Test_F1']:.4f}\")\n",
    "print(f\"     - ROC-AUC: {rf_tuned_results['Test_ROC_AUC']:.4f}\")\n",
    "print(f\"   XGBoost:\")\n",
    "print(f\"     - F1-Score: {xgb_tuned_results['Test_F1']:.4f}\")\n",
    "print(f\"     - ROC-AUC: {xgb_tuned_results['Test_ROC_AUC']:.4f}\")\n",
    "\n",
    "print(\"\\n7. KEY FINDINGS:\")\n",
    "print(\"   - Sampling techniques significantly impact model performance\")\n",
    "print(\"   - Tree-based models (RF, XGBoost) generally perform better than Logistic Regression\")\n",
    "print(\"   - Hyperparameter tuning provides measurable improvements\")\n",
    "print(\"   - SMOTE and combined techniques often yield best results for imbalanced data\")\n",
    "\n",
    "print(\"\\n8. RECOMMENDATIONS:\")\n",
    "print(f\"   - Deploy {best_model_info['Model']} with {best_model_info['Sampling']} sampling\")\n",
    "print(\"   - Monitor model performance on new data\")\n",
    "print(\"   - Consider ensemble methods for further improvement\")\n",
    "print(\"   - Regularly retrain with updated data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"END OF MODELING\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
